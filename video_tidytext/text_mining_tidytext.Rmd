---
title: "Text mining avec TidyText sous R"
output:
  html_document:
    df_print: paged
---

## Chargement et inspection de la base

#### Chargement

```{r}
#vider la mémoire
rm(list=ls())

#changer de dossier courant
# setwd("C:/Users/ricco/Desktop/demo")

#charger les packages de l'univers Tidy
#en particulier dplyr
library(tidyverse)

#charger les données
#avec le package readxl
D <- readxl::read_excel("C:\\Users\\ganci\\Documents\\master_miashs sans TER\\Analyse de données textuelles\\Projet\\Analyse_d_opinion\\video_tidytext\\imdb_reviews_1000.xlsx")
str(D)
```

```{r}
#premiers documents de la base de données
head(D)
```

```{r}
#inspection des classes
print(table(D$label))
```

#### Quelques vérifications supplémentaires

Présence de la **balise HTML "saut de ligne"** dans les commentaires.

```{r}
#présence des '<br />' dans les commentaires
tmp <- grep("<br />",D$commentaires)
print(length(tmp))
```

```{r}
#lequels
print(head(tmp))
```

```{r}
#le premier par ex.
print(D$commentaires[1])
```

```{r}
#le troisième par ex.
print(D$commentaires[3])
```

Présence de **chiffres** dans les commentaires.

```{r}
#présence des chiffres
#cf. expressions régulières sous R
#http://tutoriels-data-mining.blogspot.com/2017/01/les-expression-regulieres-sous-r.html
tmp <- grep('[0-9]',D$commentaires)
print(length(tmp))
```

```{r}
#lesquels
print(head(tmp))
```

```{r}
#le 2ème message
print(D$commentaires[2])
```

```{r}
#le premier
print(D$commentaires[1])
```

## TidyText - Premières explorations

#### Préparation des données

```{r}
#charger spécifiquement la libraire tidytext
library(tidytext)
```

La numérotation des documents permet de mieux comprendre les associations des termes avec les documents par la suite.

```{r}
#modifier la structure du tibble
#en numérotant les documents
DPRIM <- tibble(line=1:nrow(D),commentaires=D$commentaires)
print(head(DPRIM))
```

#### Tokenisation

Transformation des documents en une structure "tidy", en recensant les mots par document. L'information sur l'ordre des mots est perdue. On note en revanche que la casse a été changée, la ponctuation a été supprimée.

```{r}
#traiter le texte brut
res_A <- DPRIM %>%
  unnest_tokens(output=word,input=commentaires)

#
print(res_A)
```

#### Inspection du 1er document

```{r}
#si on s'en tient aux mots du premier document
mots_1 <- res_A %>%
  filter(line==1) %>%
  select(word)

print(mots_1$word)
```

```{r}
#pour rappel le 1er document était...
print(D$commentaires[1])
```

#### Fréquence des termes dans les documents

```{r}
#dans le premier document
mots_1 %>%
  group_by(word) %>%
  count(sort=TRUE)
```

```{r}
#comptage des termes par document
compte_A <- res_A %>%
  group_by(line,word) %>%
  summarize(freq=n())

print(compte_A)
```


#### Dictionnaire global des termes

```{r}
#dictionnaire - comptage
#ordonnancement selon la fréquence
dico_A <- res_A %>%
  count(word,sort=TRUE)

#affichage
print(dico_A)
```

```{r}
#les termes les moins fréquents
print(tail(dico_A))
```

## TidyText - Nettoyage des données

#### Stopwords (mots-vides)

Librairie pour les stopwords en anglais.

```{r}
#récupération des stopwords
data("stop_words")
print(stop_words)
```

#### Seconde version de la tokenisation

Sans les chiffres, la balise "saut de ligne" et les stopwords.

```{r}
#deuxième version, sans les chiffres et les br
res_B <- DPRIM %>%
  mutate(text=gsub(x=commentaires,pattern="[0-9]",replacement="")) %>%
  mutate(text=gsub(x=text,pattern="<br />",replacement="")) %>%
  unnest_tokens(output=word,input=text) %>%
  filter(!word %in% stop_words$word) %>%
  select(line,word)

#affichage
print(res_B)
```

```{r}
#vérification pour le premier document
print(res_B[res_B$line==1,]$word)
```

```{r}
#par comparaison
DPRIM$commentaires[1]
```

#### Dictionnaire des termes

```{r}
#dictionnaire
dico_B <- res_B %>%
  count(word,sort=TRUE)

print(dico_B)
```

Avec un petit "wordcloud" pour faire joli...

```{r}
#un petit wordcloud ici
library(wordcloud)
wordcloud(words=dico_B$word,freq=dico_B$n,max.word=50,colors = brewer.pal(8,'Dark2'))

```

#### Matrice documents-termes

Une matrice avec en ligne les documents, en colonne les termes, et en valeurs la fréquence des termes dans les documents.

```{r}
#rappel structure tidy
print(head(res_B))
```


```{r}
#comptage des termes par document
compte_B <- res_B %>%
  group_by(line,word) %>%
  summarize(freq=n())

print(compte_B)
```

```{r}
#nécessité de disposer de la librairie "tm"
library(tm)

#"cast" en MDT (pondération = fréquence)
#autre pondération possible, ex. TF-IDF
#cf. https://www.rdocumentation.org/packages/tidytext/versions/0.3.2/topics/bind_tf_idf
mdt_B <- compte_B %>%
  cast_dtm(document = line, term = word, value = freq)

#affichage
print(mdt_B)
```

```{r}
#dans un format "matrix"
#plus facile à manipuler pour nous
mat_B <- as.matrix(mdt_B)

#classe
print(class(mat_B))

#dimension
print(dim(mat_B))
```

```{r}
#comptage des termes pour le document 1
print(mat_B[1,which(mat_B[1,]>0)])
```

```{r}
#vérifions
tmp <- compte_B %>%
  filter(line==1)

#affichage
tmpvec <- tmp$freq
names(tmpvec) <- tmp$word
print(tmpvec)
```

#### Dictionnaire et fréquence des termes


```{r}
#pour rappel
head(dico_B,20)
```

Refaire le calcul à partir de la matrice documents-termes.

```{r}
print(head(sort(colSums(mat_B),decreasing=TRUE),20))
```


#### Apparition des termes dans les documents

Nombre de documents où les termes apparaissent au moins une fois.

```{r}
#comptage des doc.
app_termes <- apply(mat_B,2,function(x){sum(x>0)})

#affichage trié
print(head(sort(app_termes,decreasing=TRUE),20))
```

#### Filtrage selon la fréquence

Retirer de la matrice les termes qui apparaissent dans trop peu ou trop nombreux documents.

```{r}
#condition sur les colonnes
mat_B_filtered <- mat_B[,app_termes > 10 & app_termes <1000]

#dimensions
print(dim(mat_B_filtered))
```

## Analyse via la matrice documents-termes

Quelques pistes pour éexploiter la matrice documents-termes.

#### Polarité observée des termes

N'oublions pas que les documents sont étiquetés (commentaires positifs ou négatifs). L'idée est de déterminer si les termes sont plus souvent associés à des documents positifs ou négatifs.

On ne travaille que sur les termes filtrés, avec la pondération binaire.

```{r}
#pondération binaire
mat_C <- ifelse(mat_B_filtered>0,1,0)

#transformer la matrice en data.frame
df_C <- as.data.frame(mat_C)
print(class(df_C))
```

```{r}
#calcul conditionnel
sum_per_class <- aggregate(x=df_C,by=list(D$label),sum)
print(sum_per_class[,1:15])
```

```{r}
#structure temporaire
tmp <- as.matrix(sum_per_class[,2:ncol(sum_per_class)])
row.names(tmp) <- sum_per_class$Group.1

#proportion des positifs
prop_pos <- tmp['pos',]/colSums(tmp)
print(sort(prop_pos)[1:15])
```

```{r}
#ex. le cas de 'waste'
print(tmp[,'waste'])
```

#### Schéma explicatif - Arbres de décision

On n'est pas vraiment dans un schéma prédictif, mais plutôt dans la description. Sinon, il faudrait repenser le processus de préparation des données.

```{r}
#construire un arbre de décision
library(rpart)
arbre <- rpart(D$label ~ ., data = df_C)
print(arbre)
```

```{r}
#affichage plus sympathique
library(rpart.plot)
rpart.plot(arbre)
```
```{r}
#proportion de documents positifs pour les "pires" termes
print(prop_pos[c('bad','worst','waste','boring')])
```

## Analyse des sentiments (traditionnelle)

S'intéresser aux polarités de termes pour inférer sur la polarité des documents.

#### Lexique des sentiments

```{r}
#lexique pour l'analyse des sentiments
#polarité des termes - get_sentiments() de tidtytext
#"bing" est une source possible, il y en a d'autres
polarite_termes <- get_sentiments("bing")
print(polarite_termes)
```

#### Polarité des documents

Polarité d'un document = aggrégation de la polarité des termes qui le compose... s'ils sont recensés bien sûr. Pas besoin de partition apprentissage-test ici parce que la classe (label) n'est pas mise à contribution lors de la "modélisation". L'approche est non-supervisée par nature, pas de risque de surapprentissage.

Calcul de la polarité des documents par utilisation du lexique des sentiments. Etape par étape.

```{r}
#recensement des termes par document
res_B %>%
  filter(line==1)
```

```{r}
#pour un document, jointure avec le lexique
res_B %>%
  filter(line==1) %>%
  left_join(polarite_termes,by="word")
```

```{r}
#retrait des NA, à ne pas comptabiliser
res_B %>%
  filter(line==1) %>%
  left_join(polarite_termes,by="word") %>%
  filter(!is.na(sentiment))
```
```{r}
#comptabilisation des polarités
res_B %>%
  filter(line==1) %>%
  left_join(polarite_termes,by="word") %>%
  filter(!is.na(sentiment)) %>%
  count(sentiment)
```

Il ne reste plus qu'à réaliser l'opération sur l'ensemble des documents.

```{r}
#polarité des documents déduite des termes
#qui les composent
#--> proportion des termes positifs
pol_per_doc <- res_B %>%
  group_by(line) %>%
  left_join(polarite_termes,by="word") %>%
  filter(!is.na(sentiment)) %>%
  count(sentiment) %>%
  summarise(freq_pos=n[sentiment=='positive']/sum(n))

#affichage
print(pol_per_doc)
```

Attention, certains documents ne sont pas recensés (1916 documents notés en tout) parce qu'ils ne présentent aucun terme "positif".

Prédiction par comparaison avec le seuil 0.5 c.-à-d. est considéré comme positif un commentaire s'il est composé au moins pour moitié de termes positifs.

```{r}
#prédiction sur cette base
pred_polarite <- rep(0,nrow(D))

#affectation -- ternir compte des doc. non recensés
pred_polarite[pol_per_doc$line] <- pol_per_doc$freq_pos

#classe prédite
class_pred_polarite <- ifelse(pred_polarite >= 0.5,"positive","negative")
print(table(class_pred_polarite))
```

Confrontation avec les classes observées.

```{r}
#matrice de confusion
mc <-table(D$label,class_pred_polarite) 
print(mc)
```

```{r}
#accuracy
print(sum(diag(mc))/sum(mc))
```

