---
title: "R Notebook"
output: html_notebook
---



```{r}
#vider la mémoire
rm(list=ls())

#changer de dossier courant
# setwd("C:/Users/ricco/Desktop/demo")

#charger les packages de l'univers Tidy
#en particulier dplyr
library(tidyverse)
library(dplyr)

#charger les données
#avec le package readxl
df <- read.csv("C:\\Users\\YCarl\\OneDrive\\Bureau\\Analyse_d_opinion\\Codes\\dataQI_fichierSpacy.csv",encoding = "UTF-8", sep=";")

```


```{r}
#premiers documents de la base de données
head(df)
```
`





## TidyText - Premières explorations

#### Préparation des données

```{r}
#charger spécifiquement la libraire tidytext
library(tidytext)
```



La numérotation des documents permet de mieux comprendre les associations des termes avec les documents par la suite.

```{r}
#modifier la structure du tibble
#en numérotant les documents
DPRIM <- tibble(line=1:nrow(df),post=df$postfinal)
print(head(DPRIM))
```



#### Tokenisation

Transformation des documents en une structure "tidy", en recensant les mots par document. L'information sur l'ordre des mots est perdue. On note en revanche que la casse a été changée, la ponctuation a été supprimée.

```{r}
#traiter le texte brut
res_A <- DPRIM %>%
  unnest_tokens(output=word,input=post)

#
# print(res_A)
```




#### Inspection du 1er document

```{r}
#si on s'en tient aux mots du premier document
mots_1 <- res_A %>%
  filter(line==1) %>%
  select(word)

print(mots_1$word)
```




```{r}
#comptage des termes par document
compte_A <- res_A %>%
  group_by(line,word) %>%
  summarize(freq=n())

print(compte_A)
```


#### Dictionnaire global des termes
```{r}
#dictionnaire - comptage
#ordonnancement selon la fréquence
dico_A <- res_A %>%
  count(word,sort=TRUE)

#affichage
print(dico_A)
```



```{r}
#les termes les moins fréquents
print(tail(dico_A))
```

## TidyText - Nettoyage des données

#### Stopwords (mots-vides)

Librairie pour les stopwords en francais.

```{r}
#récupération des stopwords
library("lsa")
data(stopwords_fr)
# print(stopwords_fr)
```


#### Tokenisation finale du dataset

Sans les chiffres, la balise "saut de ligne" et les stopwords.


```{r}
#deuxième version, sans les chiffres et les br
res_B <- DPRIM %>%
  mutate(text=gsub(x=post,pattern="[0-9]",replacement="")) %>%
  mutate(text=gsub(x=post,pattern="ã",replacement="e")) %>%
  mutate(text=gsub(x=post,pattern="â",replacement="e")) %>%
  unnest_tokens(output=word,input=text) %>%
  filter(!word %in% stopwords_fr) %>%
  select(line,word)

#affichage
print(res_B)
```


```{r}
res_Bigram <- DPRIM %>%
  mutate(text=gsub(x=post,pattern="[0-9]",replacement="")) %>%
  mutate(text=gsub(x=post,pattern="ã",replacement="e")) %>%
  mutate(text=gsub(x=post,pattern="â",replacement="e")) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)


#affichage
print(res_Bigram)


```


```{r}
res_Bigram_counts <- res_Bigram %>%
  count(bigram, bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2"), sep = " ")
res_Bigram_counts
```


```{r}
negate_words <- c("ne", "pas", "non","ni")
```


```{r}
res <- res_Bigram_counts %>%
  filter(word1 %in% negate_words) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(polarite_termes, by = c(word2 = "word")) %>%
  filter(!is.na(polarity)) %>%
  mutate(contribution = n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 5) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1))
res
```


```{r}
res_Bigram_counts %>%
  filter(word1 %in% negate_words) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(polarite_termes, by = c(word2 = "word")) %>%
  filter(!is.na(polarity)) %>%
  mutate(contribution = n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 10) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = polarity)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~ word1, scales = "free", nrow = 3) +
  scale_y_reordered() +
  labs(x = "polarité par occurence de mots",
       y = "mots precedés d'une negation")
```




#### Dictionnaire des termes

```{r}
#dictionnaire
dico_B <- res_B %>%
  count(word,sort=TRUE)

print(dico_B)
```

Avec un petit "wordcloud" pour faire joli...

```{r}
#un petit wordcloud ici
library(wordcloud)
wordcloud(words=dico_B$word,freq=dico_B$n,max.word=50,colors = brewer.pal(8,'Dark2'))

```

#### Matrice documents-termes

Une matrice avec en ligne les documents, en colonne les termes, et en valeurs la fréquence des termes dans les documents.

```{r}
#rappel structure tidy
print(head(res_B))
```


```{r}
#comptage des termes par document
compte_B <- res_B %>%
  group_by(line,word) %>%
  summarize(freq=n())

print(compte_B)
```




```{r}
#nécessité de disposer de la librairie "tm"
library(tm)

#"cast" en MDT (pondération = fréquence)
#autre pondération possible, ex. TF-IDF
#cf. https://www.rdocumentation.org/packages/tidytext/versions/0.3.2/topics/bind_tf_idf
mdt_B <- compte_B %>%
  cast_dtm(document = line, term = word, value = freq)

#affichage
print(mdt_B)
```


```{r}
#dans un format "matrix"
#plus facile à manipuler pour nous
mat_B <- as.matrix(mdt_B)

#classe
print(class(mat_B))

#dimension
print(dim(mat_B))
```

#### Dictionnaire et fréquence des termes
```{r}
#pour rappel
head(dico_B,20)
```

Refaire le calcul à partir de la matrice documents-termes.

```{r}
print(head(sort(colSums(mat_B),decreasing=TRUE),20))
```


#### Apparition des termes dans les documents
Nombre de documents où les termes apparaissent au moins une fois.

```{r}
#comptage des doc.
app_termes <- apply(mat_B,2,function(x){sum(x>0)})

#affichage trié
print(head(sort(app_termes,decreasing=TRUE),20))
```

#### Filtrage selon la fréquence
Retirer de la matrice les termes qui apparaissent dans trop peu ou trop nombreux documents.

```{r}
#condition sur les colonnes
mat_B_filtered <- mat_B[,app_termes > 10 & app_termes <1000]

#dimensions
print(dim(mat_B_filtered))
```

## Analyse via la matrice documents-termes

Quelques pistes pour éexploiter la matrice documents-termes.

#### Polarité observée des termes

N'oublions pas que les documents sont étiquetés (commentaires positifs ou négatifs). L'idée est de déterminer si les termes sont plus souvent associés à des documents positifs ou négatifs.

On ne travaille que sur les termes filtrés, avec la pondération binaire.


```{r}
#pondération binaire
mat_C <- ifelse(mat_B_filtered>0,1,0)

#transformer la matrice en data.frame
df_C <- as.data.frame(mat_C)
print(class(df_C))
```


## Analyse des sentiments (traditionnelle)

S'intéresser aux polarités de termes pour inférer sur la polarité des documents.

#### Lexique des sentiments

```{r}
polarite_termes <- read.csv("C:\\Users\\ganci\\Documents\\master_miashs sans TER\\Analyse de données textuelles\\Projet\\Analyse_d_opinion\\Données\\FEEL.csv",sep = ";",encoding = "UTF-8")
print(polarite_termes)
```

#### Polarité des documents
Polarité d'un document = aggrégation de la polarité des termes qui le compose... s'ils sont recensés bien sûr. Pas besoin de partition apprentissage-test ici parce que la classe (label) n'est pas mise à contribution lors de la "modélisation". L'approche est non-supervisée par nature, pas de risque de surapprentissage.

Calcul de la polarité des documents par utilisation du lexique des sentiments. Etape par étape.

```{r}
#recensement des termes par document
res_B %>%
  filter(line==1)
```


## proportion de positivité dans un post
```{r}
#polarité des documents déduite des termes
#qui les composent
#--> proportion des termes positifs
polpos_per_doc <- res_B %>%
  group_by(line) %>%
  left_join(polarite_termes,by="word") %>%
  filter(!is.na(polarity)) %>%
  count(polarity) %>%
  summarise(freq_pos=n[polarity=='positive']/sum(n))

#affichage
print(polpos_per_doc)
```



## proportion de negativité dans un post
```{r}
#polarité des documents déduite des termes
#qui les composent
#--> proportion des termes positifs
polneg_per_doc <- res_B %>%
  group_by(line) %>%
  left_join(polarite_termes,by="word") %>%
  filter(!is.na(polarity)) %>%
  count(polarity) %>%
  summarise(freq_neg=n[polarity=='negative']/sum(n))

#affichage
print(polneg_per_doc)
```

#ajout de l'information de la polarité positive et negative de chaque pos

```{r}
p1 <- DPRIM %>%
  left_join(polpos_per_doc,by="line")

p2 <- p1 %>%
  left_join(polneg_per_doc,by="line")

#affichage
print(p2)
```


```{r}
library(reshape2)

res_B %>%
  inner_join(polarite_termes) %>%
  count(word, polarity, sort = TRUE) %>%
  acast(word ~ polarity, value.var = "n", fill = 1.5) %>%
  comparison.cloud(colors = c("red", "green"),
                   max.words = 100)
```


```{r}
pol <- res_B %>%
  left_join(polarite_termes,by="word") %>%
  group_by(word,polarity) %>%
  filter(!is.na(polarity)) %>% 
  summarise(joi = sum(joy),peur = sum(fear),tristeste= sum(sadness),haine = sum(anger),surprise = sum(surprise),des = sum(disgust))
```

```{r}
pol %>% sort(joi) %>%  filter(joi>2) %>% ggplot(aes(joi, word, fill = polarity)) +
  geom_col(show.legend = FALSE,stat="identity") +
  facet_wrap(~polarity, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```
















Attention, certains documents ne sont pas recensés (1916 documents notés en tout) parce qu'ils ne présentent aucun terme "positif".

Prédiction par comparaison avec le seuil 0.5 c.-à-d. est considéré comme positif un commentaire s'il est composé au moins pour moitié de termes positifs.

```{r}
#prédiction sur cette base
pred_polarite <- rep(0,nrow(df))

#affectation -- ternir compte des doc. non recensés
pred_polarite[pol_per_doc$line] <- pol_per_doc$freq_pos

#classe prédite
class_pred_polarite <- ifelse(pred_polarite >= 0.65,"positive","negative")
print(table(class_pred_polarite))
```


```{r}
p2 <- p2 %>% mutate(polainter=ifelse(freq_pos>= 0.60,"positive","negative"))
```


```{r}
p2 <- p2 %>% mutate(pola_final=ifelse(!is.na(polainter),polainter,"neutre"))
```

```{r}
d <- round(prop.table(table(p2$pola_final))*100,2) %>% as.data.frame()
`colnames<-`(d,c("sentiment","prop"))
colnames(d) <- c("sentiment","prop")
```

```{r}
ggplot(d,aes(sentiment, prop)) +
  geom_bar(stat="identity") +
  labs(x = "sentiment",
       y = NULL)
```

```{r}
ggplot(data=d, aes(x=sentiment, y=prop)) +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=prop), vjust=-0.3, size=3.5)+
  theme_minimal()

ggsave("resultat.png")
```



Confrontation avec les classes observées.

```{r}
#matrice de confusion
mc <-table(df$sentiment,class_pred_polarite) 
print(mc)
```

```{r}
#accuracy
print(sum(diag(mc))/sum(mc))
```



